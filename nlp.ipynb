{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "# Encoding text using bag of words\n",
    "vocab = {}  # maps word to integer representing it\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "  global word_encoding\n",
    "\n",
    "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
    "  bag = {}  # stores all of the encodings and their frequency\n",
    "\n",
    "  for word in words:\n",
    "    if word in vocab:\n",
    "      encoding = vocab[word]  # get encoding from vocab\n",
    "    else:\n",
    "      vocab[word] = word_encoding\n",
    "      encoding = word_encoding\n",
    "      word_encoding += 1\n",
    "    \n",
    "    if encoding in bag:\n",
    "      bag[encoding] += 1\n",
    "    else:\n",
    "      bag[encoding] = 1\n",
    "  \n",
    "  return bag\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "# Integer Encoding\n",
    "vocab = {}  \n",
    "word_encoding = 1\n",
    "def one_hot_encoding(text):\n",
    "  global word_encoding\n",
    "\n",
    "  words = text.lower().split(\" \") \n",
    "  encoding = []  \n",
    "\n",
    "  for word in words:\n",
    "    if word in vocab:\n",
    "      code = vocab[word]  \n",
    "      encoding.append(code) \n",
    "    else:\n",
    "      vocab[word] = word_encoding\n",
    "      encoding.append(word_encoding)\n",
    "      word_encoding += 1\n",
    "  \n",
    "  return encoding\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test a a\"\n",
    "encoding = one_hot_encoding(text)\n",
    "print(encoding)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 194,\n",
       " 1153,\n",
       " 194,\n",
       " 8255,\n",
       " 78,\n",
       " 228,\n",
       " 5,\n",
       " 6,\n",
       " 1463,\n",
       " 4369,\n",
       " 5012,\n",
       " 134,\n",
       " 26,\n",
       " 4,\n",
       " 715,\n",
       " 8,\n",
       " 118,\n",
       " 1634,\n",
       " 14,\n",
       " 394,\n",
       " 20,\n",
       " 13,\n",
       " 119,\n",
       " 954,\n",
       " 189,\n",
       " 102,\n",
       " 5,\n",
       " 207,\n",
       " 110,\n",
       " 3103,\n",
       " 21,\n",
       " 14,\n",
       " 69,\n",
       " 188,\n",
       " 8,\n",
       " 30,\n",
       " 23,\n",
       " 7,\n",
       " 4,\n",
       " 249,\n",
       " 126,\n",
       " 93,\n",
       " 4,\n",
       " 114,\n",
       " 9,\n",
       " 2300,\n",
       " 1523,\n",
       " 5,\n",
       " 647,\n",
       " 4,\n",
       " 116,\n",
       " 9,\n",
       " 35,\n",
       " 8163,\n",
       " 4,\n",
       " 229,\n",
       " 9,\n",
       " 340,\n",
       " 1322,\n",
       " 4,\n",
       " 118,\n",
       " 9,\n",
       " 4,\n",
       " 130,\n",
       " 4901,\n",
       " 19,\n",
       " 4,\n",
       " 1002,\n",
       " 5,\n",
       " 89,\n",
       " 29,\n",
       " 952,\n",
       " 46,\n",
       " 37,\n",
       " 4,\n",
       " 455,\n",
       " 9,\n",
       " 45,\n",
       " 43,\n",
       " 38,\n",
       " 1543,\n",
       " 1905,\n",
       " 398,\n",
       " 4,\n",
       " 1649,\n",
       " 26,\n",
       " 6853,\n",
       " 5,\n",
       " 163,\n",
       " 11,\n",
       " 3215,\n",
       " 10156,\n",
       " 4,\n",
       " 1153,\n",
       " 9,\n",
       " 194,\n",
       " 775,\n",
       " 7,\n",
       " 8255,\n",
       " 11596,\n",
       " 349,\n",
       " 2637,\n",
       " 148,\n",
       " 605,\n",
       " 15358,\n",
       " 8003,\n",
       " 15,\n",
       " 123,\n",
       " 125,\n",
       " 68,\n",
       " 23141,\n",
       " 6853,\n",
       " 15,\n",
       " 349,\n",
       " 165,\n",
       " 4362,\n",
       " 98,\n",
       " 5,\n",
       " 4,\n",
       " 228,\n",
       " 9,\n",
       " 43,\n",
       " 36893,\n",
       " 1157,\n",
       " 15,\n",
       " 299,\n",
       " 120,\n",
       " 5,\n",
       " 120,\n",
       " 174,\n",
       " 11,\n",
       " 220,\n",
       " 175,\n",
       " 136,\n",
       " 50,\n",
       " 9,\n",
       " 4373,\n",
       " 228,\n",
       " 8255,\n",
       " 5,\n",
       " 25249,\n",
       " 656,\n",
       " 245,\n",
       " 2350,\n",
       " 5,\n",
       " 4,\n",
       " 9837,\n",
       " 131,\n",
       " 152,\n",
       " 491,\n",
       " 18,\n",
       " 46151,\n",
       " 32,\n",
       " 7464,\n",
       " 1212,\n",
       " 14,\n",
       " 9,\n",
       " 6,\n",
       " 371,\n",
       " 78,\n",
       " 22,\n",
       " 625,\n",
       " 64,\n",
       " 1382,\n",
       " 9,\n",
       " 8,\n",
       " 168,\n",
       " 145,\n",
       " 23,\n",
       " 4,\n",
       " 1690,\n",
       " 15,\n",
       " 16,\n",
       " 4,\n",
       " 1355,\n",
       " 5,\n",
       " 28,\n",
       " 6,\n",
       " 52,\n",
       " 154,\n",
       " 462,\n",
       " 33,\n",
       " 89,\n",
       " 78,\n",
       " 285,\n",
       " 16,\n",
       " 145,\n",
       " 95]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at one review\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making train and test the same length\n",
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 11s 16ms/step - loss: 0.4283 - acc: 0.8067 - val_loss: 0.3356 - val_acc: 0.8592\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.2427 - acc: 0.9038 - val_loss: 0.2842 - val_acc: 0.8856\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.1803 - acc: 0.9334 - val_loss: 0.2849 - val_acc: 0.8860\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.1495 - acc: 0.9463 - val_loss: 0.2875 - val_acc: 0.8840\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.1290 - acc: 0.9547 - val_loss: 0.2961 - val_acc: 0.8936\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 6ms/step - loss: 0.3771 - acc: 0.8628\n",
      "[0.3770900368690491, 0.8627600073814392]\n"
     ]
    }
   ],
   "source": [
    "# Evaluated data\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "# making predictions, need to encode text to the same data as before\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# while were at it lets make a decode function\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "      if num != PAD:\n",
    "        text += reverse_word_index[num] + \" \"\n",
    "\n",
    "    return text[:-1]\n",
    "  \n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89792436]\n",
      "[0.2778695]\n"
     ]
    }
   ],
   "source": [
    "# now time to make a prediction\n",
    "\n",
    "def predict(text):\n",
    "  encoded_text = encode_text(text)\n",
    "  pred = np.zeros((1,250))\n",
    "  pred[0] = encoded_text\n",
    "  result = model.predict(pred) \n",
    "  print(result[0])\n",
    "\n",
    "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Play generator\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading file\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "vocab = sorted(set(text))\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# lets look at how part of our text is encoded\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# convert numeric values to text\n",
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])\n",
    "\n",
    "print(\"Text:\",int_to_text(text_as_int[:13]))\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training examples\n",
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn stream of characters into batches of desired length\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sequences into input and output\n",
    "def split_input_target(chunk):  # for the example: hello\n",
    "    input_text = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print(\"\\n\\nEXAMPLE\\n\")\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"\\nOUTPUT\")\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training batches\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Creating a Loss Function\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 1.6750925e-04  2.9949772e-03 -3.2634435e-03 ...  1.7700054e-03\n",
      "   -3.3006400e-03 -4.2938464e-03]\n",
      "  [ 2.0768566e-03  1.8246891e-03 -4.3698987e-03 ...  1.0308770e-03\n",
      "   -3.0184514e-03 -4.7110617e-03]\n",
      "  [ 3.1972053e-03  2.2145533e-03 -3.4424802e-03 ... -1.7368386e-03\n",
      "   -9.2933560e-04  4.2157276e-03]\n",
      "  ...\n",
      "  [ 5.6856778e-03 -5.2796234e-03  2.9977770e-03 ... -2.8356989e-03\n",
      "    6.4860755e-03 -1.4472140e-03]\n",
      "  [ 2.3558913e-03  2.1647720e-04 -5.2193692e-04 ...  1.8250858e-03\n",
      "    3.6065197e-03 -7.4210751e-05]\n",
      "  [-2.7668229e-03  2.2691994e-03  8.2934210e-03 ...  5.8786459e-03\n",
      "    8.1866619e-04  1.7116538e-03]]\n",
      "\n",
      " [[ 2.1273349e-03  1.6227949e-03  4.2960835e-03 ... -2.1441185e-03\n",
      "    3.0003081e-05 -2.2111759e-03]\n",
      "  [ 8.0583710e-04 -4.9071712e-04  6.7434581e-03 ... -4.7183684e-03\n",
      "    2.3298408e-03  4.5099752e-03]\n",
      "  [-4.2391475e-04 -1.7265291e-03  6.4124744e-03 ... -2.6627551e-03\n",
      "    3.7124110e-03  2.0994756e-03]\n",
      "  ...\n",
      "  [ 8.3589442e-03  4.3370817e-03 -1.9613984e-03 ... -9.4750093e-04\n",
      "   -1.9931274e-03  7.5129997e-03]\n",
      "  [ 9.7938895e-04  4.7460422e-03  7.8082127e-03 ...  6.1696116e-03\n",
      "   -3.2186201e-03  6.8769762e-03]\n",
      "  [ 2.0671124e-03  1.9911067e-03  3.8972846e-03 ...  5.2167000e-03\n",
      "   -1.4806646e-03  3.8705471e-03]]\n",
      "\n",
      " [[ 6.0813199e-03 -2.3779138e-03 -2.6433242e-03 ...  6.8782782e-04\n",
      "   -1.8497522e-03 -9.7880121e-03]\n",
      "  [ 6.2743695e-03 -2.2568740e-03 -4.1263346e-03 ... -4.4151430e-04\n",
      "   -1.4969038e-03 -7.5055822e-03]\n",
      "  [ 6.5937564e-03 -4.9717324e-03 -2.4140084e-03 ... -5.9670811e-03\n",
      "   -1.9382890e-03 -8.6938757e-03]\n",
      "  ...\n",
      "  [ 7.1520833e-03  1.9552039e-03  1.1684926e-02 ...  3.7326370e-03\n",
      "   -3.3701856e-03 -9.0553882e-03]\n",
      "  [ 1.5394758e-03  2.1404545e-03  1.7315131e-02 ...  8.7227281e-03\n",
      "   -4.7374796e-03 -6.1189095e-03]\n",
      "  [-1.3760445e-03  4.6027820e-03  1.4183620e-02 ...  8.6033493e-03\n",
      "   -1.1805099e-03 -1.7123111e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.7400656e-03  5.8699370e-04 -7.6906849e-04 ... -2.0162594e-03\n",
      "    1.9532777e-03  7.9791564e-03]\n",
      "  [ 3.8271644e-03 -9.1416901e-04 -2.5870190e-03 ... -1.8053290e-03\n",
      "    1.2650774e-03  4.5773257e-03]\n",
      "  [ 1.7565179e-03 -7.7207070e-03 -1.0307527e-03 ... -8.9413032e-04\n",
      "    3.6965767e-03  3.7321772e-03]\n",
      "  ...\n",
      "  [ 4.3193912e-03 -8.4909033e-03 -2.6304442e-03 ... -2.9100981e-03\n",
      "   -9.2859846e-04  4.7209638e-04]\n",
      "  [ 9.2466231e-03 -9.7979493e-03 -4.1577714e-03 ... -1.2859525e-03\n",
      "   -1.7642770e-03 -9.7833974e-03]\n",
      "  [ 8.5384734e-03 -8.6828787e-03 -4.6413061e-03 ... -1.7548469e-03\n",
      "   -7.8417477e-04 -7.8392504e-03]]\n",
      "\n",
      " [[ 1.8981177e-03 -6.0171797e-04 -2.7740290e-03 ... -6.1714719e-04\n",
      "   -1.4166255e-04 -1.3300172e-03]\n",
      "  [ 6.3212728e-04 -1.6836610e-03  2.3904855e-03 ... -2.0317638e-03\n",
      "    4.6116188e-03  4.2963810e-03]\n",
      "  [-9.5853582e-04 -9.0599582e-03  3.1895796e-04 ... -3.8494342e-03\n",
      "    4.0882379e-03  5.6198840e-03]\n",
      "  ...\n",
      "  [ 6.1909854e-03 -8.6261500e-03  1.9709796e-03 ...  1.7920677e-03\n",
      "    4.9236617e-03 -2.0614648e-03]\n",
      "  [ 8.9566931e-03 -2.4462477e-03 -3.2302202e-04 ...  3.7033337e-03\n",
      "   -2.5829696e-03 -4.1674618e-03]\n",
      "  [ 5.5792308e-03 -1.5466295e-03 -5.1594111e-03 ... -4.9830147e-04\n",
      "   -3.3669479e-03 -2.9169964e-03]]\n",
      "\n",
      " [[-1.4338912e-03 -8.9073917e-03 -1.8698475e-03 ... -1.7443339e-03\n",
      "   -8.0166152e-05  2.8133392e-03]\n",
      "  [-5.4592919e-03 -5.3998549e-03  6.5357839e-03 ...  3.9508762e-03\n",
      "   -3.4769350e-03  2.7667093e-03]\n",
      "  [ 2.2623416e-03 -7.3840874e-03  2.4195274e-03 ...  3.4549700e-03\n",
      "   -4.8300084e-03 -8.1723658e-03]\n",
      "  ...\n",
      "  [-1.0486715e-03  4.2270967e-03  7.0360852e-03 ...  4.9132248e-03\n",
      "    3.0555371e-03 -9.5560532e-03]\n",
      "  [ 3.4284231e-03  5.3329896e-03  5.6735170e-03 ...  3.6184625e-03\n",
      "   -3.9535919e-03 -1.0797495e-02]\n",
      "  [ 2.4805986e-03 -2.0822855e-03  8.6926213e-03 ...  2.6514516e-03\n",
      "   -8.9901779e-04 -1.0851753e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 1.6750925e-04  2.9949772e-03 -3.2634435e-03 ...  1.7700054e-03\n",
      "  -3.3006400e-03 -4.2938464e-03]\n",
      " [ 2.0768566e-03  1.8246891e-03 -4.3698987e-03 ...  1.0308770e-03\n",
      "  -3.0184514e-03 -4.7110617e-03]\n",
      " [ 3.1972053e-03  2.2145533e-03 -3.4424802e-03 ... -1.7368386e-03\n",
      "  -9.2933560e-04  4.2157276e-03]\n",
      " ...\n",
      " [ 5.6856778e-03 -5.2796234e-03  2.9977770e-03 ... -2.8356989e-03\n",
      "   6.4860755e-03 -1.4472140e-03]\n",
      " [ 2.3558913e-03  2.1647720e-04 -5.2193692e-04 ...  1.8250858e-03\n",
      "   3.6065197e-03 -7.4210751e-05]\n",
      " [-2.7668229e-03  2.2691994e-03  8.2934210e-03 ...  5.8786459e-03\n",
      "   8.1866619e-04  1.7116538e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 0.00016751  0.00299498 -0.00326344  0.00203141 -0.00481923  0.00127132\n",
      "  0.00220486 -0.00478442  0.0044174  -0.00327809 -0.00456909 -0.00178141\n",
      " -0.0017141  -0.00145762  0.00106    -0.00120031  0.00087855 -0.00044071\n",
      " -0.00281738  0.00176086 -0.00114317 -0.00384362 -0.00371323 -0.00146628\n",
      " -0.00493713 -0.00253771  0.00417714 -0.00778077 -0.0004183  -0.00093889\n",
      " -0.00159036  0.00322513  0.00437075  0.00309451  0.00100923 -0.00428651\n",
      " -0.00276882  0.00015249  0.00037626 -0.00214973 -0.00346406 -0.0027049\n",
      " -0.00339789 -0.00176939  0.00041107 -0.00330927  0.00178166 -0.00122692\n",
      "  0.0024228   0.00012841 -0.00633711 -0.00027688  0.00639298 -0.00163627\n",
      "  0.00114704 -0.00314441 -0.00043513  0.00133852  0.0068442   0.00102606\n",
      " -0.00621608  0.00230392  0.00177001 -0.00330064 -0.00429385], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally well look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rEB;TqsAnsWUlfGNZeTdQCeRJrMVDYv?NTmVizpA$lK xXlV- l'OSS\\nQmqXN;xfcnq:mD!PxoMe\\nzgeavuf&k!HLi,elgVdz$Hw\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating checkpoints\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 2.6286\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.9013\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.6484\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 10s 58ms/step - loss: 1.5170\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 1.4366\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(data, epochs=5, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27672/985288127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcheckpoint_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./training_checkpoints/ckpt_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorEnv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorEnv\\lib\\site-packages\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mis_hdf5_filepath\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[0;32m    323\u001b[0m           filepath.endswith('.hdf5'))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.util._pywrap_checkpoint_reader.C' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "checkpoint_num = 10\n",
    "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text\n",
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeoth once, you for his mother's like\n",
      "UScound all cortain with wise!\n",
      "The trink that struck a maid they so the dward\n",
      "and night behold you the ore of king!\n",
      "But comes so fire your strenged but all ochain.\n",
      "\n",
      "LADY ANNE:\n",
      "You since this take you find Lord Henry?\n",
      "\n",
      "GORZALO:\n",
      "The lain o, a, the lack\n",
      "Ahong revenge, he is you from her life:\n",
      "If fire is forws, the earth levier the back.\n",
      "\n",
      "KATHARINA:\n",
      "Musingly look you; five, I'll take them father,\n",
      "What I undill'd the slaughter'd thingsy\n",
      "This world in duppion'd balliath'd:\n",
      "This in dusy? Yet?\n",
      "\n",
      "ANGBUL:\n",
      "Why, sit it is me well: aly is the fire,\n",
      "As bring for heart at as a miserion of like known'd\n",
      "Than the loving digding that boy!\n",
      "But it is my leave of Kings Richard,\n",
      "If foul thee, Within couse to hel; I no, my marm rich to be marriang,\n",
      "Who enepys have a\n",
      "vising deadle\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "894bed9f287e6644623875e503a0c8695e4ecd2dd5dd2368222c217ac6149a9e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tensorEnv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
