{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Tensorflow on Device\n",
    "Assuming hardware is compatible:\n",
    "1. Install latest NVIDIA drivers\n",
    "2. Install CUDA Toolkit\n",
    "3. Install cuDNN SDK and copy files to CUDA directories <br>\n",
    "{unzipped dir}/bin/ --> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin <br>\n",
    "{unzipped dir}/include/ --> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\include <br>\n",
    "{unzipped dir}/lib/ --> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module, check if it gpu is linked\n",
    "import tensorflow as tf\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional packages\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import urllib\n",
    "\n",
    "import tensorflow.compat.v2.feature_column as fc\n",
    "import tensorflow_probability as tfp\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
    "# seperate the response variable\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode dataset to work with tensor flow\n",
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                       'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column\n",
    "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
    "\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input function so that we can convert pandas df into a tf.data.Dataset object\n",
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "  def input_function():  # inner function, this will be returned\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(1000)  # randomize order of data\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
    "    return ds  # return a batch of the dataset\n",
    "  return input_function  # return a function object for use\n",
    "\n",
    "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
    "# We create a linear estimtor by passing the feature columns we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model, and evaluating it\n",
    "linear_est.train(train_input_fn)  # train\n",
    "result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data\n",
    "\n",
    "print(result)  # the result variable is simply a dict of stats about our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction probably rounds the probabilities to the nearest classifier (based on some threshold, usually .5) to compute accuracy.\n",
    "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "# Lets define some constants to help us later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\n",
      "16384/2194 [================================================================================================================================================================================================================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\n",
      "16384/573 [=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "train_path = tf.keras.utils.get_file(\n",
    "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_path = tf.keras.utils.get_file(\n",
    "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "0          6.4         2.8          5.6         2.2        2\n",
       "1          5.0         2.3          3.3         1.0        1\n",
       "2          4.9         2.5          4.5         1.7        2\n",
       "3          4.9         3.1          1.5         0.1        0\n",
       "4          5.7         3.8          1.7         0.3        0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1\n",
       "4          5.7         3.8          1.7         0.3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "train.head() # the species column is now gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape  # we have 120 entires with 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input function\n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "print(my_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Aaron\\AppData\\Local\\Temp\\tmpjnegq4fj\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Aaron\\\\AppData\\\\Local\\\\Temp\\\\tmpjnegq4fj', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 30 and 10 nodes respectively.\n",
    "    hidden_units=[30, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Aaron\\AppData\\Local\\Temp\\tmpjnegq4fj\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 1.1628675, step = 0\n",
      "INFO:tensorflow:global_step/sec: 227.272\n",
      "INFO:tensorflow:loss = 0.9904677, step = 100 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.069\n",
      "INFO:tensorflow:loss = 0.9415838, step = 200 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.067\n",
      "INFO:tensorflow:loss = 0.91165125, step = 300 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.756\n",
      "INFO:tensorflow:loss = 0.8782464, step = 400 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.808\n",
      "INFO:tensorflow:loss = 0.85945946, step = 500 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.41\n",
      "INFO:tensorflow:loss = 0.82319933, step = 600 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.116\n",
      "INFO:tensorflow:loss = 0.80378586, step = 700 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.718\n",
      "INFO:tensorflow:loss = 0.7848983, step = 800 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.247\n",
      "INFO:tensorflow:loss = 0.76727206, step = 900 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.666\n",
      "INFO:tensorflow:loss = 0.75135255, step = 1000 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.818\n",
      "INFO:tensorflow:loss = 0.7405212, step = 1100 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.43\n",
      "INFO:tensorflow:loss = 0.7226512, step = 1200 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.191\n",
      "INFO:tensorflow:loss = 0.70320755, step = 1300 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.158\n",
      "INFO:tensorflow:loss = 0.6939969, step = 1400 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.972\n",
      "INFO:tensorflow:loss = 0.6845594, step = 1500 (0.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.236\n",
      "INFO:tensorflow:loss = 0.66661835, step = 1600 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.806\n",
      "INFO:tensorflow:loss = 0.6690177, step = 1700 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.857\n",
      "INFO:tensorflow:loss = 0.6550327, step = 1800 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.056\n",
      "INFO:tensorflow:loss = 0.6463337, step = 1900 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.165\n",
      "INFO:tensorflow:loss = 0.6399852, step = 2000 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.138\n",
      "INFO:tensorflow:loss = 0.6251879, step = 2100 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.103\n",
      "INFO:tensorflow:loss = 0.6055885, step = 2200 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.381\n",
      "INFO:tensorflow:loss = 0.6103567, step = 2300 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.732\n",
      "INFO:tensorflow:loss = 0.60238874, step = 2400 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.394\n",
      "INFO:tensorflow:loss = 0.5955257, step = 2500 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.039\n",
      "INFO:tensorflow:loss = 0.59359926, step = 2600 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 263.158\n",
      "INFO:tensorflow:loss = 0.5822912, step = 2700 (0.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.38\n",
      "INFO:tensorflow:loss = 0.57034236, step = 2800 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.78\n",
      "INFO:tensorflow:loss = 0.56866276, step = 2900 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.097\n",
      "INFO:tensorflow:loss = 0.562293, step = 3000 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.361\n",
      "INFO:tensorflow:loss = 0.5542089, step = 3100 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 267.379\n",
      "INFO:tensorflow:loss = 0.55902785, step = 3200 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.509\n",
      "INFO:tensorflow:loss = 0.54780775, step = 3300 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.038\n",
      "INFO:tensorflow:loss = 0.53612536, step = 3400 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.55\n",
      "INFO:tensorflow:loss = 0.5470035, step = 3500 (0.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.41\n",
      "INFO:tensorflow:loss = 0.52194333, step = 3600 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.792\n",
      "INFO:tensorflow:loss = 0.5260061, step = 3700 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.07\n",
      "INFO:tensorflow:loss = 0.51280326, step = 3800 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.738\n",
      "INFO:tensorflow:loss = 0.50610346, step = 3900 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.458\n",
      "INFO:tensorflow:loss = 0.49930438, step = 4000 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.381\n",
      "INFO:tensorflow:loss = 0.49879462, step = 4100 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.78\n",
      "INFO:tensorflow:loss = 0.5022474, step = 4200 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.085\n",
      "INFO:tensorflow:loss = 0.48518032, step = 4300 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.068\n",
      "INFO:tensorflow:loss = 0.47452667, step = 4400 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.754\n",
      "INFO:tensorflow:loss = 0.48283952, step = 4500 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.467\n",
      "INFO:tensorflow:loss = 0.4837411, step = 4600 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.416\n",
      "INFO:tensorflow:loss = 0.47115415, step = 4700 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.453\n",
      "INFO:tensorflow:loss = 0.46907616, step = 4800 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.164\n",
      "INFO:tensorflow:loss = 0.46051565, step = 4900 (0.394 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\Aaron\\AppData\\Local\\Temp\\tmpjnegq4fj\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n",
      "INFO:tensorflow:Loss for final step: 0.46174416.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x22581ff0130>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps=5000)\n",
    "# We include a lambda to avoid creating an inner function previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-11-07T12:09:13\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Aaron\\AppData\\Local\\Temp\\tmpjnegq4fj\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.15318s\n",
      "INFO:tensorflow:Finished evaluation at 2021-11-07-12:09:14\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.8666667, average_loss = 0.5370217, global_step = 5000, loss = 0.5370217\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: C:\\Users\\Aaron\\AppData\\Local\\Temp\\tmpjnegq4fj\\model.ckpt-5000\n",
      "\n",
      "Test set accuracy: 0.867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type numeric values as prompted.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Aaron\\AppData\\Local\\Temp\\tmpjnegq4fj\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Prediction is \"Virginica\" (88.5%)\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "def input_fn(features, batch_size=256):\n",
    "    # Convert the inputs to a Dataset without labels.\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
    "predict = {}\n",
    "\n",
    "print(\"Please type numeric values as prompted.\")\n",
    "for feature in features:\n",
    "  valid = True\n",
    "  while valid: \n",
    "    val = input(feature + \": \")\n",
    "    if not val.isdigit(): valid = False\n",
    "\n",
    "  predict[feature] = [float(val)]\n",
    "\n",
    "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
    "for pred_dict in predictions:\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
    "        SPECIES[class_id], 100 * probability))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "Predict furture events based on passed events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Model\n",
    "Taken direclty from the TensorFlow documentation (https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/HiddenMarkovModel). \n",
    "\n",
    "We will model a simple weather system and try to predict the temperature on each day given the following information.\n",
    "1. Cold days are encoded by a 0 and hot days are encoded by a 1.\n",
    "2. The first day in our sequence has an 80% chance of being cold.\n",
    "3. A cold day has a 30% chance of being followed by a hot day.\n",
    "4. A hot day has a 20% chance of being followed by a cold day.\n",
    "5. On each day the temperature is\n",
    " normally distributed with mean and standard deviation 0 and 5 on\n",
    " a cold day and mean and standard deviation 15 and 10 on a hot day.\n",
    "\n",
    "If you're unfamiliar with **standard deviation** it can be put simply as the range of expected values. \n",
    "\n",
    "In this example, on a hot day the average temperature is 15 and ranges from 5 to 25.\n",
    "\n",
    "To model this in TensorFlow we will do the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions  # making a shortcut for later on\n",
    "initial_distribution = tfd.Categorical(probs=[0.2, 0.8])  # Refer to point 2 above\n",
    "transition_distribution = tfd.Categorical(probs=[[0.5, 0.5], [0.2, 0.8]])  # refer to points 3 and 4 above\n",
    "observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above\n",
    "\n",
    "# the loc argument represents the mean and the scale is the standard devitation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27dbf42753113e11682fcc5c3e03739fe099a36ec37fd1a95add47902f2adca1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('dataScienceEnv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
